{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50849b9e",
   "metadata": {},
   "source": [
    "\n",
    "# El Algoritmo del Perceptrón y su Relación con el Entrenamiento\n",
    "\n",
    "En esta sección se presenta el perceptrón como el modelo lineal más básico, junto con su regla de actualización inspirada en el gradiente para ajustar pesos y sesgo. Además, se introducen los conceptos de gradiente descendente, su regla de actualización y algunas variantes empleadas en la optimización de modelos de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857f957",
   "metadata": {},
   "source": [
    "\n",
    "## El Perceptrón: Modelo Lineal Básico y Regla de Actualización\n",
    "\n",
    "El perceptrón es el modelo lineal más básico utilizado para la clasificación. Su salida se calcula mediante la combinación lineal de las entradas:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i \\, x_i + b,\n",
    "$$\n",
    "\n",
    "y se determina la salida a partir de una función escalón:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1, & \\text{si } z \\geq 0, \\\\\n",
    "0, & \\text{si } z < 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Para ajustar los pesos $w_i$ y el sesgo $b$, se emplea una regla de actualización que se aplica de la siguiente forma para cada muestra:\n",
    " \n",
    "$$\n",
    "w_i \\leftarrow w_i + \\eta \\, (y - \\hat{y}) \\, x_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "b \\leftarrow b + \\eta \\, (y - \\hat{y}),\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\eta$ es la tasa de aprendizaje,\n",
    "- $y$ es la etiqueta real,\n",
    "- $\\hat{y}$ es la salida predicha.\n",
    "\n",
    "Esta regla de actualización es similar al mecanismo del gradiente descendente, ya que se ajustan los parámetros en función del error cometido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc64f7",
   "metadata": {},
   "source": [
    "\n",
    "## Gradiente Descendente: Fundamentos y Conceptos\n",
    "\n",
    "El gradiente descendente es un algoritmo de optimización que se utiliza para minimizar una función de pérdida $J(\\theta)$, donde $\\theta$ representa los parámetros del modelo. El método consiste en actualizar iterativamente los parámetros en la dirección opuesta al gradiente de la función de pérdida. La regla de actualización se expresa como:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\theta$ puede representar un peso o un sesgo,\n",
    "- $\\eta$ es la tasa de aprendizaje,\n",
    "- $\\nabla_\\theta J(\\theta)$ es el gradiente de la función de pérdida con respecto a $\\theta$.\n",
    "\n",
    "Este método permite que el modelo se acerque gradualmente al mínimo de la función de pérdida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75f208",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo Sencillo: Minimización de $J(w) = w^2$\n",
    "\n",
    "Consideremos la función de pérdida\n",
    "\n",
    "$$\n",
    "J(w) = w^2.\n",
    "$$\n",
    "\n",
    "El gradiente de $J(w)$ con respecto a $w$ es:\n",
    "\n",
    "$$\n",
    "\\frac{dJ}{dw} = 2w.\n",
    "$$\n",
    "\n",
    "La regla de actualización del gradiente descendente para minimizar $J(w)$ es:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot 2w.\n",
    "$$\n",
    "\n",
    "A continuación se muestra un ejemplo en Python que ilustra cómo se utiliza el gradiente descendente para minimizar esta función.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Minimización de J(w) = w^2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definición de la función de pérdida J(w) = w^2\n",
    "def J(w):\n",
    "    return w**2\n",
    "\n",
    "# Derivada de J(w)\n",
    "def grad_J(w):\n",
    "    return 2 * w\n",
    "\n",
    "# Parámetros iniciales\n",
    "w = 10.0           # Valor inicial de w\n",
    "eta = 0.1          # Tasa de aprendizaje\n",
    "epochs = 20        # Número de iteraciones\n",
    "\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "# Aplicar la regla de actualización del gradiente descendente\n",
    "for epoch in range(epochs):\n",
    "    grad = grad_J(w)\n",
    "    w = w - eta * grad\n",
    "    w_history.append(w)\n",
    "    loss_history.append(J(w))\n",
    "    print(f\"Época {epoch+1:2d}: w = {w:.4f}, J(w) = {J(w):.4f}\")\n",
    "\n",
    "# Visualizar la evolución de J(w)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), loss_history, marker='o')\n",
    "plt.title(\"Evolución de J(w) durante el entrenamiento\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"J(w)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe5d29",
   "metadata": {},
   "source": [
    "\n",
    "## Variantes del Gradiente Descendente y Optimización Avanzada\n",
    "\n",
    "Existen diversas variantes del gradiente descendente que se adaptan a distintos escenarios:\n",
    "\n",
    "- **Gradiente Descendente Batch:**  \n",
    "  Se calcula el gradiente usando todo el conjunto de datos antes de actualizar los parámetros. Esto puede ser costoso computacionalmente para conjuntos de datos grandes.\n",
    "\n",
    "- **Gradiente Descendente Estocástico (SGD):**  \n",
    "  Se actualizan los parámetros utilizando cada muestra individualmente. Esto introduce ruido en las actualizaciones, lo que puede ayudar a escapar de óptimos locales, aunque puede hacer que la convergencia sea menos estable.\n",
    "\n",
    "- **Gradiente Descendente Mini-Batch:**  \n",
    "  Se divide el conjunto de datos en pequeños lotes (mini-batches) y se actualizan los parámetros usando el gradiente calculado en cada uno. Esta variante es la más común, ya que equilibra la estabilidad y la eficiencia computacional.\n",
    "\n",
    "Además, se han desarrollado optimizadores avanzados que incorporan técnicas para acelerar la convergencia:\n",
    "- **Momentum:**  \n",
    "  Introduce un término que acumula la dirección del gradiente de iteraciones anteriores, lo que ayuda a suavizar las actualizaciones y acelerar el proceso.\n",
    "- **RMSProp:**  \n",
    "  Ajusta la tasa de aprendizaje de cada parámetro de forma adaptativa, basándose en el promedio de los cuadrados de los gradientes.\n",
    "- **Adam (Adaptive Moment Estimation):**  \n",
    "  Combina las ideas de Momentum y RMSProp para proporcionar actualizaciones eficientes y robustas. Adam es uno de los optimizadores más utilizados en el aprendizaje profundo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf550e15",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Cómo Funciona el Gradiente Descendente Batch?\n",
    "\n",
    "El gradiente descendente Batch utiliza el conjunto completo de datos para calcular la función de pérdida y su gradiente. En cada iteración, se procesan todas las muestras disponibles para obtener el gradiente promedio, el cual se utiliza para actualizar los parámetros del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8453973",
   "metadata": {},
   "source": [
    "\n",
    "### Cálculo de la Función de Pérdida Global\n",
    "\n",
    "Para un conjunto de datos con $m$ muestras, si se utiliza el error cuadrático medio (MSE) en un problema de regresión, la función de pérdida se define como:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2,\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $h(x_i)$ es la predicción del modelo para la muestra $i$,\n",
    "- $y_i$ es el valor real para la muestra $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdba6e6",
   "metadata": {},
   "source": [
    "\n",
    "### Cálculo del Gradiente del Conjunto Completo\n",
    "\n",
    "El gradiente del parámetro $w$ se obtiene promediando los gradientes individuales de cada muestra:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w} = \\frac{1}{m}\\sum_{i=1}^{m} (h(x_i) - y_i)x_i.\n",
    "$$\n",
    "\n",
    "Esto significa que se toma la media de todos los gradientes calculados, asegurando que la dirección del descenso refleje la totalidad de la información disponible en el dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370543c",
   "metadata": {},
   "source": [
    "\n",
    "### Actualización de los Parámetros\n",
    "\n",
    "Una vez obtenido el gradiente promedio, los parámetros se actualizan utilizando la regla:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\, \\frac{\\partial J(w)}{\\partial w},\n",
    "$$\n",
    "\n",
    "donde $\\eta$ representa la tasa de aprendizaje. Esta actualización se efectúa en cada iteración utilizando todos los datos del conjunto, lo que garantiza que la dirección del descenso sea precisa y estable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38dbdc",
   "metadata": {},
   "source": [
    "\n",
    "### Ejemplo Intuitivo\n",
    "\n",
    "Considera un conjunto de datos con 1000 muestras. Para actualizar el parámetro $w$, el algoritmo realiza lo siguiente en cada iteración:\n",
    "- Calcula el error y el gradiente para cada una de las 1000 muestras.\n",
    "- Promedia estos 1000 gradientes para obtener una dirección única de actualización.\n",
    "- Actualiza $w$ usando este gradiente promedio.\n",
    "\n",
    "Este enfoque asegura que cada actualización refleje la totalidad del conjunto de datos, lo que conduce a una convergencia suave y estable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33236679",
   "metadata": {},
   "source": [
    "\n",
    "### Resumen\n",
    "\n",
    "- **Batch Gradient Descent** utiliza el conjunto completo de datos para calcular el gradiente.\n",
    "- Cada actualización se basa en el gradiente promedio, lo que ofrece una dirección de descenso precisa y estable.\n",
    "- Este método es adecuado para conjuntos de datos pequeños o medianos, pero puede resultar computacionalmente costoso para datasets muy grandes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Batch Gradient Descent para Regresión Lineal con múltiples muestras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir el conjunto de datos: 5 muestras\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y = 2 * X  # La relación verdadera es y = 2x\n",
    "\n",
    "def cost_batch(w, X, y):\n",
    "    m = len(X)\n",
    "    return 0.5 * np.mean((w * X - y) ** 2)\n",
    "\n",
    "def grad_cost_batch(w, X, y):\n",
    "    m = len(X)\n",
    "    return np.mean((w * X - y) * X)\n",
    "\n",
    "# Inicialización del parámetro\n",
    "w = 0.0\n",
    "eta = 0.01  # Tasa de aprendizaje\n",
    "epochs = 200\n",
    "\n",
    "w_history_batch = []\n",
    "cost_history_batch = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Calcula el gradiente usando el conjunto completo (Batch)\n",
    "    grad = grad_cost_batch(w, X, y)\n",
    "    # Actualiza el parámetro\n",
    "    w = w - eta * grad\n",
    "    w_history_batch.append(w)\n",
    "    cost_history_batch.append(cost_batch(w, X, y))\n",
    "\n",
    "print(\"Batch Gradient Descent:\")\n",
    "print(\"Valor final de w:\", w)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), cost_history_batch, marker='o', markersize=3, label=\"Costo\")\n",
    "plt.title(\"Evolución del Costo en Batch Gradient Descent\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1976679",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Cómo Funciona el Gradiente Descendente Estocástico (SGD)?\n",
    "\n",
    "El gradiente descendente estocástico (SGD) utiliza una única muestra de datos para calcular el gradiente en cada actualización. Esto implica que, en lugar de procesar todas las muestras del dataset como en el método Batch, se selecciona una muestra (o un pequeño subconjunto) en cada paso para calcular el gradiente, lo que conduce a actualizaciones más frecuentes pero también más ruidosas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e5630",
   "metadata": {},
   "source": [
    "\n",
    "### Cálculo del Gradiente en SGD\n",
    "\n",
    "Para una muestra $i$, el gradiente se estima como:\n",
    "\n",
    "$$\n",
    "\\nabla J(w) \\approx (h(x_i) - y_i)x_i,\n",
    "$$\n",
    "\n",
    "donde $h(x_i)$ es la predicción del modelo para la muestra $i$ y $y_i$ es su valor real. Cada actualización utiliza únicamente este gradiente, lo que puede provocar variabilidad en la dirección del descenso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57155c8",
   "metadata": {},
   "source": [
    "\n",
    "### Actualización de los Parámetros en SGD\n",
    "\n",
    "Con una tasa de aprendizaje $\\eta$, el parámetro $w$ se actualiza para cada muestra según:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\, (h(x_i) - y_i)x_i.\n",
    "$$\n",
    "\n",
    "Debido a que se realiza una actualización por cada muestra, en cada época se efectúan $m$ actualizaciones (siendo $m$ el número de muestras), lo que permite respuestas rápidas a cambios en los datos, pero a la vez introduce mayor ruido en las actualizaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6254262",
   "metadata": {},
   "source": [
    "\n",
    "### Ejemplo Intuitivo de SGD\n",
    "\n",
    "Considérese un conjunto de datos con $m$ muestras. En cada época, el algoritmo:\n",
    "- Baraja las muestras.\n",
    "- Calcula el gradiente para cada muestra de forma individual.\n",
    "- Actualiza los parámetros de forma inmediata usando el gradiente calculado.\n",
    "\n",
    "Este proceso se repite durante múltiples épocas. Aunque cada actualización individual puede ser imprecisa debido a la alta varianza, el proceso global permite al modelo converger a lo largo del tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d95285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Stochastic Gradient Descent para Regresión Lineal con múltiples muestras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir el conjunto de datos: 5 muestras\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y = 2 * X  # La relación verdadera es y = 2x\n",
    "\n",
    "def cost_sgd(w, X, y):\n",
    "    m = len(X)\n",
    "    return 0.5 * np.mean((w * X - y) ** 2)\n",
    "\n",
    "def grad_cost_sample(w, x_i, y_i):\n",
    "    # Calcula el gradiente para una sola muestra\n",
    "    return (w * x_i - y_i) * x_i\n",
    "\n",
    "w = 0.0\n",
    "eta = 0.01\n",
    "epochs = 20  # Se usan más épocas debido al ruido en las actualizaciones\n",
    "\n",
    "w_history_sgd = []\n",
    "cost_history_sgd = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Barajar los índices de las muestras en cada época\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for i in indices:\n",
    "        # Actualizar el parámetro para cada muestra individual\n",
    "        grad = grad_cost_sample(w, X[i], y[i])\n",
    "        w = w - eta * grad\n",
    "    w_history_sgd.append(w)\n",
    "    cost_history_sgd.append(cost_sgd(w, X, y))\n",
    "    print(f\"Epoch {epoch+1:2d}: w = {w:.4f}, Cost = {cost_history_sgd[-1]:.4f}\")\n",
    "\n",
    "print(\"Stochastic Gradient Descent:\")\n",
    "print(\"Valor final de w:\", w)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), cost_history_sgd, marker='o', markersize=5, label=\"Costo\")\n",
    "plt.title(\"Evolución del Costo en Stochastic Gradient Descent\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a45ac0",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Comparación entre Batch y Stochastic Gradient Descent\n",
    "- **Batch Gradient Descent:** Calcula el gradiente utilizando todas las muestras, lo que resulta en una actualización por época basada en el gradiente promedio. Esto proporciona actualizaciones precisas y estables, pero puede ser computacionalmente costoso para grandes conjuntos de datos.\n",
    "- **Stochastic Gradient Descent (SGD):** Actualiza los parámetros utilizando una sola muestra a la vez, lo que produce actualizaciones más frecuentes y puede ayudar a escapar de óptimos locales, aunque introduce mayor ruido en la dirección del descenso.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee334e2",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Cómo Funciona el Gradiente Descendente Mini-Batch?\n",
    "\n",
    "El gradiente descendente mini-batch combina aspectos del método Batch y del Stochastic Gradient Descent (SGD). En este enfoque, se divide el conjunto de datos completo en pequeños grupos llamados *mini-batches*. Para cada mini-batch se calcula el gradiente promedio, y luego se actualizan los parámetros del modelo. De este modo se logra un equilibrio entre la estabilidad de las actualizaciones basadas en todo el conjunto y la rapidez de las actualizaciones basadas en muestras individuales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f5614",
   "metadata": {},
   "source": [
    "\n",
    "### Cálculo del Gradiente en Mini-Batch\n",
    "\n",
    "Para un mini-batch que contiene $B$ muestras, la función de pérdida se calcula como:\n",
    "\n",
    "$$\n",
    "J_B(w) = \\frac{1}{2B}\\sum_{i \\in \\text{batch}}(h(x_i) - y_i)^2,\n",
    "$$\n",
    "\n",
    "y el gradiente se estima promediando sobre las muestras del mini-batch:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_B(w)}{\\partial w} = \\frac{1}{B}\\sum_{i \\in \\text{batch}}(h(x_i) - y_i)x_i.\n",
    "$$\n",
    "\n",
    "Esta estimación se utiliza para actualizar los parámetros con la tasa de aprendizaje $\\eta$:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\, \\frac{\\partial J_B(w)}{\\partial w}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea276af3",
   "metadata": {},
   "source": [
    "\n",
    "### Ejemplo Intuitivo de Mini-Batch Gradient Descent\n",
    "\n",
    "Considere un conjunto de datos de $m$ muestras. En cada iteración, el dataset se divide en mini-batches de tamaño $B$. Para cada mini-batch, se calcula el gradiente promedio y se actualizan los parámetros. Este enfoque permite obtener actualizaciones más frecuentes que el método Batch y reduce la variabilidad en comparación con el SGD, logrando así un balance entre precisión y velocidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Mini-Batch Gradient Descent para Regresión Lineal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir el conjunto de datos: 10 muestras\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=float)\n",
    "y = 2 * X  # La relación verdadera es y = 2x\n",
    "\n",
    "def cost_minibatch(w, X, y):\n",
    "    B = len(X)\n",
    "    return 0.5 * np.mean((w * X - y) ** 2)\n",
    "\n",
    "def grad_cost_minibatch(w, X_batch, y_batch):\n",
    "    return np.mean((w * X_batch - y_batch) * X_batch)\n",
    "\n",
    "# Inicialización del parámetro\n",
    "w = 0.0\n",
    "eta = 0.01      # Tasa de aprendizaje\n",
    "epochs = 200\n",
    "batch_size = 3  # Tamaño del mini-batch\n",
    "\n",
    "w_history_minibatch = []\n",
    "cost_history_minibatch = []\n",
    "\n",
    "m = len(X)\n",
    "for epoch in range(epochs):\n",
    "    # Barajar los índices de las muestras\n",
    "    indices = np.arange(m)\n",
    "    np.random.shuffle(indices)\n",
    "    # Procesar el dataset en mini-batches\n",
    "    for start in range(0, m, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_indices = indices[start:end]\n",
    "        X_batch = X[batch_indices]\n",
    "        y_batch = y[batch_indices]\n",
    "        grad = grad_cost_minibatch(w, X_batch, y_batch)\n",
    "        w = w - eta * grad\n",
    "    w_history_minibatch.append(w)\n",
    "    cost_history_minibatch.append(cost_minibatch(w, X, y))\n",
    "\n",
    "print(\"Mini-Batch Gradient Descent:\")\n",
    "print(\"Valor final de w:\", w)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), cost_history_minibatch, marker='o', markersize=3, label=\"Costo\")\n",
    "plt.title(\"Evolución del Costo en Mini-Batch Gradient Descent\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077afe40",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Comparación: Mini-Batch vs. Batch vs. SGD\n",
    "- **Batch Gradient Descent:** Calcula el gradiente utilizando el conjunto completo de datos, lo que produce actualizaciones precisas y estables, pero con una sola actualización por época.\n",
    "- **Stochastic Gradient Descent (SGD):** Actualiza los parámetros utilizando una sola muestra, lo que genera actualizaciones muy frecuentes pero con alta variabilidad en la dirección del gradiente.\n",
    "- **Mini-Batch Gradient Descent:** Divide el conjunto de datos en mini-batches, ofreciendo un compromiso entre estabilidad y frecuencia de actualizaciones. Es el método más utilizado en la práctica, ya que combina la precisión de Batch y la rapidez de SGD.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57823191",
   "metadata": {},
   "source": [
    "\n",
    "## Optimizers en Redes Neuronales\n",
    "\n",
    "Los optimizadores son algoritmos que se utilizan para ajustar los parámetros (por ejemplo, pesos y sesgos) de un modelo, con el fin de minimizar una función de pérdida. Durante el entrenamiento de una red neuronal, se busca encontrar el conjunto de parámetros que reduzca al mínimo el error entre las predicciones del modelo y los valores reales. Estos algoritmos se basan en métodos de gradiente descendente y sus variantes, y juegan un papel crucial en la eficiencia y efectividad del entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b1b6c",
   "metadata": {},
   "source": [
    "\n",
    "### ¿Qué Hacen los Optimizers?\n",
    "\n",
    "Los optimizadores realizan las siguientes tareas:\n",
    "- **Calcular el Gradiente:** Evalúan la derivada de la función de pérdida respecto a cada parámetro del modelo.\n",
    "- **Actualizar Parámetros:** Modifican los parámetros en la dirección opuesta al gradiente para reducir el error.\n",
    "- **Gestionar la Tasa de Aprendizaje:** Ajustan la magnitud de las actualizaciones, ya sea mediante una tasa fija o de forma adaptativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a129d2a",
   "metadata": {},
   "source": [
    "\n",
    "### Tipos Comunes de Optimizadores\n",
    "\n",
    "A continuación se describen algunos de los optimizadores más utilizados en el entrenamiento de redes neuronales:\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent):**\n",
    "  - Actualiza los parámetros utilizando una muestra a la vez.\n",
    "  - Es simple y requiere poca memoria, pero puede presentar actualizaciones ruidosas y converger lentamente.\n",
    "\n",
    "- **Momentum:**\n",
    "  - Incorpora un término que acumula el gradiente de iteraciones previas para suavizar y acelerar la convergencia.\n",
    "  - Ayuda a superar oscilaciones en el proceso de actualización.\n",
    "\n",
    "- **RMSProp:**\n",
    "  - Ajusta la tasa de aprendizaje de cada parámetro de forma individual, basándose en el promedio de los cuadrados de los gradientes.\n",
    "  - Es efectivo para tratar problemas con gradientes de diferente escala.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation):**\n",
    "  - Combina las ideas de Momentum y RMSProp para ofrecer actualizaciones robustas y eficientes.\n",
    "  - Es uno de los optimizadores más populares en el aprendizaje profundo debido a su capacidad para adaptarse a distintos escenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d39fb1",
   "metadata": {},
   "source": [
    "\n",
    "### Ventajas y Desventajas de los Optimizadores\n",
    "\n",
    "Cada optimizador tiene sus propias características:\n",
    "\n",
    "- **SGD:**\n",
    "  - *Ventajas:* Simple de implementar, bajo consumo de memoria.\n",
    "  - *Desventajas:* Convergencia lenta y alta variabilidad en las actualizaciones.\n",
    "\n",
    "- **Momentum:**\n",
    "  - *Ventajas:* Acelera la convergencia y reduce las oscilaciones.\n",
    "  - *Desventajas:* Requiere un ajuste cuidadoso del hiperparámetro de momentum.\n",
    "\n",
    "- **RMSProp:**\n",
    "  - *Ventajas:* Gestiona eficazmente tasas de aprendizaje variables para cada parámetro.\n",
    "  - *Desventajas:* Involucra hiperparámetros adicionales que deben ajustarse para cada problema.\n",
    "\n",
    "- **Adam:**\n",
    "  - *Ventajas:* Combina las ventajas de Momentum y RMSProp, resultando robusto y eficiente en muchos escenarios.\n",
    "  - *Desventajas:* Puede ser sensible a la elección de la tasa de aprendizaje y otros hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2842e8",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Consejo\n",
    "Los optimizadores son esenciales en el entrenamiento de redes neuronales. Permiten ajustar los parámetros del modelo de forma automatizada y eficiente. Se recomienda experimentar con distintos optimizadores y ajustar sus hiperparámetros para encontrar la configuración óptima para cada problema. La elección del optimizador puede influir significativamente en la velocidad de convergencia y en la calidad final del modelo.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5b785",
   "metadata": {},
   "source": [
    "\n",
    "## Momentum\n",
    "\n",
    "Momentum es una técnica de optimización que acelera el gradiente descendente en la dirección de descenso relevante y reduce las oscilaciones. Esto se logra incorporando el concepto de \"velocidad\", que es el promedio móvil exponencial de los gradientes pasados. En lugar de actualizar los parámetros únicamente con el gradiente actual, se acumula una porción del gradiente de iteraciones anteriores.\n",
    "\n",
    "La regla de actualización con Momentum se expresa de la siguiente manera:\n",
    "\n",
    "$$\n",
    "v_t = \\beta \\, v_{t-1} + (1 - \\beta) \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, v_t,\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\theta$ representa los parámetros del modelo.\n",
    "- $\\eta$ es la tasa de aprendizaje.\n",
    "- $v_t$ es la \"velocidad\" o acumulación del gradiente en el instante $t$.\n",
    "- $\\beta$ es el hiperparámetro de momentum, cuyo valor típicamente se encuentra entre 0.9 y 0.99.\n",
    "- $\\nabla_\\theta J(\\theta)$ es el gradiente de la función de pérdida respecto a los parámetros.\n",
    "\n",
    "Esta formulación permite que, cuando los gradientes tienen una dirección consistente a lo largo de varias iteraciones, la \"velocidad\" $v_t$ se incremente y se acelere la convergencia. Al mismo tiempo, ayuda a suavizar las actualizaciones en zonas donde el gradiente es ruidoso o varía mucho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45b89d",
   "metadata": {},
   "source": [
    "\n",
    "### Ventajas y Desventajas de Momentum\n",
    "\n",
    "**Ventajas:**\n",
    "- **Aceleración:** Al acumular gradientes pasados, Momentum acelera la convergencia en direcciones consistentes.\n",
    "- **Reducción de Oscilaciones:** Ayuda a reducir las oscilaciones en regiones con gradientes ruidosos, lo que conduce a actualizaciones más estables.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Hiperparámetro Adicional:** Requiere ajustar el valor de $\\beta$, lo que puede añadir complejidad al proceso de tuning.\n",
    "- **Posible Exceso de Inercia:** Si $\\beta$ es demasiado alto, el modelo puede \"sobreacelerarse\" y pasar por alto el mínimo de la función de pérdida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a39ada",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Consejo\n",
    "El uso de Momentum resulta especialmente beneficioso cuando el gradiente descendente tradicional presenta oscilaciones o converge de forma lenta. Se recomienda experimentar con valores de $\\beta$ en el rango de 0.9 a 0.99 para encontrar la configuración óptima que acelere la convergencia sin inducir inestabilidad.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2883ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Minimización de J(w) = w^2 utilizando Momentum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función de pérdida J(w) = w^2\n",
    "def J(w):\n",
    "    return w**2\n",
    "\n",
    "# Definir la derivada de J(w)\n",
    "def grad_J(w):\n",
    "    return 2 * w\n",
    "\n",
    "# Inicialización del parámetro\n",
    "w = 10.0         # Valor inicial de w\n",
    "eta = 0.1        # Tasa de aprendizaje\n",
    "beta = 0.9       # Hiperparámetro de momentum\n",
    "epochs = 50      # Número de iteraciones\n",
    "\n",
    "# Inicialización de la velocidad (momentum acumulado)\n",
    "v = 0.0\n",
    "\n",
    "# Listas para almacenar la evolución de w y el costo\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Calcular el gradiente actual\n",
    "    grad = grad_J(w)\n",
    "    # Actualizar el momentum: se acumula el gradiente ponderado por beta\n",
    "    v = beta * v + (1 - beta) * grad\n",
    "    # Actualizar el parámetro utilizando el valor acumulado en v\n",
    "    w = w - eta * v\n",
    "    w_history.append(w)\n",
    "    loss_history.append(J(w))\n",
    "    print(f\"Epoch {epoch+1:2d}: w = {w:.4f}, Loss = {J(w):.4f}\")\n",
    "\n",
    "# Visualizar la evolución del costo a lo largo de las iteraciones\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), loss_history, marker='o', markersize=5)\n",
    "plt.title(\"Evolución del Costo con Momentum\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo J(w)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f92a10",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Observación sobre el Aumento de la Función de Pérdida con Momentum\n",
    "Si en el ejemplo de Momentum la función de pérdida aumenta en lugar de disminuir, puede ser un indicativo de que los hiperparámetros no están bien ajustados. En particular, podrían estar sucediendo las siguientes situaciones:\n",
    "- **Tasa de Aprendizaje ($\\eta$) demasiado alta:**  \n",
    "  Esto provoca actualizaciones demasiado agresivas, lo que puede llevar a que el parámetro $w$ sobrepase el mínimo de la función de pérdida. Como resultado, el valor de $J(w)$ puede aumentar en lugar de disminuir.\n",
    "- **Valor de Momentum ($\\beta$) demasiado elevado:**  \n",
    "  Un valor de $\\beta$ excesivamente alto puede acumular demasiada \"inercia\" de los gradientes pasados, haciendo que las actualizaciones sean demasiado grandes y provoquen oscilaciones o divergencia.\n",
    "  \n",
    "En tales casos, se recomienda experimentar reduciendo la tasa de aprendizaje y/o ajustando el valor del hiperparámetro de momentum para lograr una convergencia más estable y evitar que la función de pérdida aumente.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Minimización de J(w) = w^2 utilizando Momentum con tasa de aprendizaje ajustada\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función de pérdida J(w) = w^2\n",
    "def J(w):\n",
    "    return w**2\n",
    "\n",
    "# Definir la derivada de J(w)\n",
    "def grad_J(w):\n",
    "    return 2 * w\n",
    "\n",
    "# Inicialización del parámetro\n",
    "w = 10.0         # Valor inicial de w\n",
    "eta = 0.01       # Tasa de aprendizaje ajustada (más pequeña para evitar actualizaciones demasiado agresivas)\n",
    "beta = 0.9       # Hiperparámetro de momentum\n",
    "epochs = 50      # Número de iteraciones\n",
    "\n",
    "# Inicialización de la velocidad (momentum acumulado)\n",
    "v = 0.0\n",
    "\n",
    "# Listas para almacenar la evolución de w y del costo\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Calcular el gradiente actual\n",
    "    grad = grad_J(w)\n",
    "    # Actualizar el momentum: acumula el gradiente ponderado por beta\n",
    "    v = beta * v + (1 - beta) * grad\n",
    "    # Actualizar el parámetro utilizando el valor acumulado en v\n",
    "    w = w - eta * v\n",
    "    w_history.append(w)\n",
    "    loss_history.append(J(w))\n",
    "    print(f\"Epoch {epoch+1:2d}: w = {w:.4f}, Loss = {J(w):.4f}\")\n",
    "\n",
    "# Visualizar la evolución del costo a lo largo de las iteraciones\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), loss_history, marker='o', markersize=5)\n",
    "plt.title(\"Evolución del Costo con Momentum (Tasa Ajustada)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo J(w)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo interactivo utilizando ipywidgets para Momentum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "def run_momentum(eta, beta, epochs, w_init):\n",
    "    # Inicializar el parámetro y la velocidad\n",
    "    w = w_init\n",
    "    v = 0.0\n",
    "    w_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    # Ejecutar el algoritmo de Momentum durante 'epochs' iteraciones\n",
    "    for epoch in range(epochs):\n",
    "        grad = 2 * w  # Derivada de J(w) = w^2\n",
    "        # Actualización del momentum: se acumula parte del gradiente pasado y el gradiente actual\n",
    "        v = beta * v + (1 - beta) * grad\n",
    "        # Actualización del parámetro w\n",
    "        w = w - eta * v\n",
    "        w_history.append(w)\n",
    "        loss_history.append(w**2)\n",
    "    \n",
    "    # Configurar la visualización de la evolución de w y del costo\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), w_history, marker='o', color='blue')\n",
    "    plt.title(\"Evolución de w\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"w\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), loss_history, marker='o', color='red')\n",
    "    plt.title(\"Evolución de la Pérdida J(w)\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"J(w)\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crear los widgets interactivos para ajustar los hiperparámetros\n",
    "interact(run_momentum,\n",
    "         eta=FloatSlider(value=0.01, min=0.001, max=0.1, step=0.001, description='Learning Rate'),\n",
    "         beta=FloatSlider(value=0.9, min=0.0, max=0.99, step=0.01, description='Momentum'),\n",
    "         epochs=IntSlider(value=50, min=10, max=200, step=10, description='Epochs'),\n",
    "         w_init=FloatSlider(value=10.0, min=-20.0, max=20.0, step=0.5, description='w Init'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7101f74",
   "metadata": {},
   "source": [
    "\n",
    "## RMSProp\n",
    "\n",
    "RMSProp es un optimizador que ajusta la tasa de aprendizaje de cada parámetro de forma individual mediante el cálculo de un promedio móvil de los cuadrados de los gradientes. En lugar de utilizar una tasa de aprendizaje fija para todos los parámetros, RMSProp escala la actualización en función de la magnitud reciente de los gradientes, lo que puede mejorar la estabilidad y la velocidad de convergencia en problemas con gradientes de escalas variables.\n",
    "\n",
    "La regla de actualización de RMSProp se expresa de la siguiente manera:\n",
    "\n",
    "$$\n",
    "v_t = \\gamma \\, v_{t-1} + (1 - \\gamma) \\, \\left(\\nabla_\\theta J(\\theta)\\right)^2,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\, \\nabla_\\theta J(\\theta),\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\theta$ representa los parámetros del modelo.\n",
    "- $\\eta$ es la tasa de aprendizaje.\n",
    "- $v_t$ es el promedio móvil de los gradientes al cuadrado en el instante $t$.\n",
    "- $\\gamma$ es el factor de decaimiento (típicamente en torno a 0.9).\n",
    "- $\\epsilon$ es una pequeña constante que se añade para evitar la división por cero.\n",
    "- $\\nabla_\\theta J(\\theta)$ es el gradiente de la función de pérdida respecto a los parámetros.\n",
    "\n",
    "Esta técnica normaliza las actualizaciones, reduciendo las oscilaciones y permitiendo que el optimizador se adapte a gradientes de diferentes magnitudes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d6f3d",
   "metadata": {},
   "source": [
    "\n",
    "### Ventajas y Desventajas de RMSProp\n",
    "\n",
    "**Ventajas:**\n",
    "- Ajusta la tasa de aprendizaje para cada parámetro, lo que puede acelerar la convergencia en problemas donde los gradientes tienen escalas muy distintas.\n",
    "- Reduce las oscilaciones en la actualización de los parámetros al normalizar los gradientes.\n",
    "\n",
    "**Desventajas:**\n",
    "- Introduce hiperparámetros adicionales ($\\gamma$ y $\\epsilon$) que requieren ajuste.\n",
    "- Puede ser sensible a la elección de estos hiperparámetros, lo que influye en la estabilidad y la velocidad de convergencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e5159",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Consejo sobre RMSProp\n",
    "RMSProp es especialmente útil en escenarios donde los gradientes varían significativamente en magnitud. Se recomienda comenzar con un valor de $\\gamma$ alrededor de 0.9 y un valor pequeño para $\\epsilon$ (por ejemplo, $1 \\times 10^{-8}$) y ajustar estos parámetros según el comportamiento observado durante el entrenamiento.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo en Python: Minimización de J(w) = w^2 utilizando RMSProp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función de pérdida J(w) = w^2\n",
    "def J(w):\n",
    "    return w**2\n",
    "\n",
    "# Definir la derivada de J(w)\n",
    "def grad_J(w):\n",
    "    return 2 * w\n",
    "\n",
    "# Inicialización del parámetro\n",
    "w = 10.0         # Valor inicial de w\n",
    "eta = 0.01       # Tasa de aprendizaje\n",
    "gamma = 0.9      # Factor de decaimiento para el promedio de los gradientes cuadrados\n",
    "epsilon = 1e-8   # Constante para estabilidad numérica\n",
    "epochs = 50      # Número de iteraciones\n",
    "\n",
    "# Inicialización del acumulador de gradientes cuadrados\n",
    "v = 0.0\n",
    "\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    grad = grad_J(w)\n",
    "    # Actualización de RMSProp: acumula el cuadrado del gradiente\n",
    "    v = gamma * v + (1 - gamma) * (grad ** 2)\n",
    "    # Actualización del parámetro usando la tasa ajustada por el promedio de los gradientes cuadrados\n",
    "    w = w - eta / (np.sqrt(v + epsilon)) * grad\n",
    "    w_history.append(w)\n",
    "    loss_history.append(J(w))\n",
    "    print(f\"Epoch {epoch+1:2d}: w = {w:.4f}, Loss = {J(w):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), loss_history, marker='o', markersize=5)\n",
    "plt.title(\"Evolución del Costo con RMSProp\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo J(w)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo interactivo utilizando ipywidgets para RMSProp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "def run_rmsprop(eta, gamma, epsilon, epochs, w_init):\n",
    "    w = w_init\n",
    "    v = 0.0\n",
    "    w_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        grad = 2 * w  # Derivada de J(w) = w^2\n",
    "        v = gamma * v + (1 - gamma) * (grad ** 2)\n",
    "        w = w - eta / (np.sqrt(v + epsilon)) * grad\n",
    "        w_history.append(w)\n",
    "        loss_history.append(w**2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), w_history, marker='o', color='blue')\n",
    "    plt.title(\"Evolución de w con RMSProp\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"w\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), loss_history, marker='o', color='red')\n",
    "    plt.title(\"Evolución de J(w) con RMSProp\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"J(w)\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(run_rmsprop,\n",
    "         eta=FloatSlider(value=0.01, min=0.001, max=0.1, step=0.001, description='Learning Rate'),\n",
    "         gamma=FloatSlider(value=0.9, min=0.0, max=0.99, step=0.01, description='Decay (γ)'),\n",
    "         epsilon=FloatSlider(value=1e-8, min=1e-10, max=1e-6, step=1e-10, description='Epsilon', readout_format='.0e'),\n",
    "         epochs=IntSlider(value=50, min=10, max=200, step=10, description='Epochs'),\n",
    "         w_init=FloatSlider(value=10.0, min=-20.0, max=20.0, step=0.5, description='w Init'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7bd3ad",
   "metadata": {},
   "source": [
    "\n",
    "## Adam Optimizer\n",
    "\n",
    "Adam (Adaptive Moment Estimation) es un optimizador que combina las ventajas de Momentum y RMSProp. Calcula tasas de aprendizaje adaptativas para cada parámetro utilizando promedios móviles exponenciales de los gradientes (primer momento) y de los cuadrados de los gradientes (segundo momento). Esta combinación permite obtener actualizaciones robustas y eficientes durante el entrenamiento.\n",
    "\n",
    "La actualización en Adam se realiza con las siguientes ecuaciones:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2) \\, \\left(\\nabla_\\theta J(\\theta)\\right)^2\n",
    "$$\n",
    "\n",
    "Se aplican correcciones de sesgo para obtener:\n",
    "\n",
    "$$\n",
    "\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "Y la actualización final de los parámetros es:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\, \\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}} + \\epsilon}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $\\theta$ representa los parámetros del modelo.\n",
    "- $\\eta$ es la tasa de aprendizaje.\n",
    "- $\\beta_1$ es el coeficiente para el primer momento, típicamente alrededor de 0.9.\n",
    "- $\\beta_2$ es el coeficiente para el segundo momento, típicamente cerca de 0.999.\n",
    "- $\\epsilon$ es una constante pequeña (por ejemplo, $10^{-8}$) para evitar la división por cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a356d6",
   "metadata": {},
   "source": [
    "\n",
    "### Ventajas y Desventajas de Adam\n",
    "\n",
    "**Ventajas:**\n",
    "- Combina las ventajas de Momentum y RMSProp, lo que resulta en actualizaciones adaptativas y aceleración de la convergencia.\n",
    "- Se adapta de forma automática a diferentes escalas de gradientes, lo que lo hace robusto en diversos escenarios.\n",
    "- En la práctica, requiere menos ajuste de hiperparámetros y suele funcionar bien con valores predeterminados.\n",
    "\n",
    "**Desventajas:**\n",
    "- Es sensible a la elección de la tasa de aprendizaje y otros hiperparámetros, aunque generalmente se ajustan a valores comunes.\n",
    "- El cálculo de promedios móviles y correcciones de sesgo añade complejidad computacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c729f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: w = 9.9000, Loss = 98.0100\n",
      "Epoch  2: w = 9.8000, Loss = 96.0405\n",
      "Epoch  3: w = 9.7001, Loss = 94.0920\n",
      "Epoch  4: w = 9.6002, Loss = 92.1646\n",
      "Epoch  5: w = 9.5005, Loss = 90.2588\n",
      "Epoch  6: w = 9.4008, Loss = 88.3748\n",
      "Epoch  7: w = 9.3012, Loss = 86.5129\n",
      "Epoch  8: w = 9.2018, Loss = 84.6735\n",
      "Epoch  9: w = 9.1026, Loss = 82.8568\n",
      "Epoch 10: w = 9.0035, Loss = 81.0629\n",
      "Epoch 11: w = 8.9046, Loss = 79.2923\n",
      "Epoch 12: w = 8.8060, Loss = 77.5450\n",
      "Epoch 13: w = 8.7075, Loss = 75.8212\n",
      "Epoch 14: w = 8.6094, Loss = 74.1213\n",
      "Epoch 15: w = 8.5115, Loss = 72.4452\n",
      "Epoch 16: w = 8.4139, Loss = 70.7931\n",
      "Epoch 17: w = 8.3166, Loss = 69.1652\n",
      "Epoch 18: w = 8.2196, Loss = 67.5615\n",
      "Epoch 19: w = 8.1229, Loss = 65.9822\n",
      "Epoch 20: w = 8.0267, Loss = 64.4273\n",
      "Epoch 21: w = 7.9308, Loss = 62.8969\n",
      "Epoch 22: w = 7.8352, Loss = 61.3910\n",
      "Epoch 23: w = 7.7401, Loss = 59.9095\n",
      "Epoch 24: w = 7.6454, Loss = 58.4526\n",
      "Epoch 25: w = 7.5512, Loss = 57.0201\n",
      "Epoch 26: w = 7.4573, Loss = 55.6120\n",
      "Epoch 27: w = 7.3640, Loss = 54.2284\n",
      "Epoch 28: w = 7.2711, Loss = 52.8690\n",
      "Epoch 29: w = 7.1787, Loss = 51.5339\n",
      "Epoch 30: w = 7.0868, Loss = 50.2229\n",
      "Epoch 31: w = 6.9954, Loss = 48.9359\n",
      "Epoch 32: w = 6.9046, Loss = 47.6729\n",
      "Epoch 33: w = 6.8142, Loss = 46.4335\n",
      "Epoch 34: w = 6.7244, Loss = 45.2178\n",
      "Epoch 35: w = 6.6352, Loss = 44.0255\n",
      "Epoch 36: w = 6.5465, Loss = 42.8565\n",
      "Epoch 37: w = 6.4584, Loss = 41.7106\n",
      "Epoch 38: w = 6.3708, Loss = 40.5876\n",
      "Epoch 39: w = 6.2839, Loss = 39.4873\n",
      "Epoch 40: w = 6.1975, Loss = 38.4095\n",
      "Epoch 41: w = 6.1118, Loss = 37.3539\n",
      "Epoch 42: w = 6.0266, Loss = 36.3205\n",
      "Epoch 43: w = 5.9421, Loss = 35.3088\n",
      "Epoch 44: w = 5.8582, Loss = 34.3187\n",
      "Epoch 45: w = 5.7749, Loss = 33.3500\n",
      "Epoch 46: w = 5.6923, Loss = 32.4024\n",
      "Epoch 47: w = 5.6103, Loss = 31.4756\n",
      "Epoch 48: w = 5.5290, Loss = 30.5694\n",
      "Epoch 49: w = 5.4483, Loss = 29.6835\n",
      "Epoch 50: w = 5.3682, Loss = 28.8177\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ejemplo en Python: Minimización de J(w) = w^2 utilizando Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir la función de pérdida J(w) = w^2\n",
    "def J(w):\n",
    "    return w**2\n",
    "\n",
    "# Definir la derivada de J(w)\n",
    "def grad_J(w):\n",
    "    return 2 * w\n",
    "\n",
    "# Inicialización del parámetro y de los momentos\n",
    "w = 10.0          # Valor inicial de w\n",
    "eta = 0.1         # Tasa de aprendizaje\n",
    "beta1 = 0.9       # Coeficiente para el primer momento\n",
    "beta2 = 0.999     # Coeficiente para el segundo momento\n",
    "epsilon = 1e-8    # Constante para evitar división por cero\n",
    "epochs = 50       # Número de iteraciones\n",
    "\n",
    "m = 0.0  # Inicialización del primer momento\n",
    "v = 0.0  # Inicialización del segundo momento\n",
    "\n",
    "w_history = []\n",
    "loss_history = []\n",
    "\n",
    "for t in range(1, epochs+1):\n",
    "    grad = grad_J(w)\n",
    "    \n",
    "    # Actualización de los momentos\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "    \n",
    "    # Corrección de sesgo\n",
    "    m_hat = m / (1 - beta1**t)\n",
    "    v_hat = v / (1 - beta2**t)\n",
    "    \n",
    "    # Actualización del parámetro\n",
    "    w = w - eta * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    \n",
    "    w_history.append(w)\n",
    "    loss_history.append(J(w))\n",
    "    print(f\"Epoch {t:2d}: w = {w:.4f}, Loss = {J(w):.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, epochs+1), loss_history, marker='o', markersize=5)\n",
    "plt.title(\"Evolución del Costo con Adam\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Costo J(w)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358cc251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adeca470cf1e4d0e846a4e8ecdca6967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='Learning Rate', max=0.5, min=0.001, step=0.001), Flo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_adam(eta, beta1, beta2, epsilon, epochs, w_init)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo interactivo utilizando ipywidgets para Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "def run_adam(eta, beta1, beta2, epsilon, epochs, w_init):\n",
    "    w = w_init\n",
    "    m = 0.0\n",
    "    v = 0.0\n",
    "    w_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    for t in range(1, epochs+1):\n",
    "        grad = 2 * w  # Derivada de J(w) = w^2\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        v_hat = v / (1 - beta2**t)\n",
    "        w = w - eta * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        w_history.append(w)\n",
    "        loss_history.append(w**2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), w_history, marker='o', color='blue')\n",
    "    plt.title(\"Evolución de w con Adam\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"w\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), loss_history, marker='o', color='red')\n",
    "    plt.title(\"Evolución de J(w) con Adam\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"J(w)\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(run_adam,\n",
    "         eta=FloatSlider(value=0.1, min=0.001, max=0.5, step=0.001, description='Learning Rate'),\n",
    "         beta1=FloatSlider(value=0.9, min=0.0, max=0.99, step=0.01, description='Beta1'),\n",
    "         beta2=FloatSlider(value=0.999, min=0.9, max=0.9999, step=0.0001, description='Beta2'),\n",
    "         epsilon=FloatSlider(value=1e-8, min=1e-10, max=1e-6, step=1e-10, description='Epsilon', readout_format='.0e'),\n",
    "         epochs=IntSlider(value=50, min=10, max=200, step=10, description='Epochs'),\n",
    "         w_init=FloatSlider(value=10.0, min=-20.0, max=20.0, step=0.5, description='w Init'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb694b0",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Consejo\n",
    "En esta sección se ha explicado el funcionamiento del perceptrón como el modelo lineal más básico, junto con su regla de actualización, la cual se inspira en el gradiente descendente. Se ha descrito el gradiente descendente como un método para minimizar funciones de pérdida mediante la actualización iterativa de los parámetros, y se ha mostrado un ejemplo sencillo con $J(w)=w^2$ para ilustrar este proceso.  \n",
    "\n",
    "Asimismo, se han comentado las variantes del gradiente descendente (batch, estocástico y mini-batch) y se han introducido optimizadores avanzados como Momentum, RMSProp y Adam, que permiten mejorar la convergencia y el rendimiento del entrenamiento en problemas complejos.\n",
    "\n",
    "Se recomienda experimentar con estos conceptos y variantes para comprender cómo afectan al proceso de optimización en modelos de aprendizaje automático.\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
