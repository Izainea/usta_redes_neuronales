{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22370d21",
   "metadata": {},
   "source": [
    "\n",
    "# Modelo Matemático de la Neurona y Funciones de Activación\n",
    "\n",
    "En esta sección se explica en detalle el modelo matemático utilizado para representar una neurona artificial y se analizan las funciones de activación empleadas para transformar la combinación lineal de entradas en una salida procesable. Se describe la fórmula utilizada, junto con las propiedades, ventajas y desventajas de funciones como la sigmoide, ReLU, tanh y softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d9706",
   "metadata": {},
   "source": [
    "\n",
    "## Modelo Matemático de la Neurona\n",
    "\n",
    "Una neurona artificial se representa mediante la siguiente combinación lineal:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i \\, x_i + b\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- **$x_i$** representa cada una de las entradas, que pueden ser características o datos provenientes del conjunto de datos.\n",
    "- **$w_i$** es el peso asignado a cada entrada, indicando la importancia de $x_i$ en el proceso de activación.\n",
    "- **$b$** es el sesgo, que permite ajustar el umbral de activación.\n",
    "- **$n$** es el número total de entradas.\n",
    "\n",
    "Una vez calculado $z$, se aplica una función de activación $f(z)$ para obtener la salida final:\n",
    "\n",
    "$$\n",
    "a = f(z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c2f56",
   "metadata": {},
   "source": [
    "\n",
    "## Funciones de Activación\n",
    "\n",
    "A continuación se explican las funciones de activación utilizadas, con sus fórmulas, propiedades, ventajas y desventajas.\n",
    "\n",
    "### Función Sigmoide\n",
    "\n",
    "La función sigmoide se define como:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Ventajas:**\n",
    "- Transforma cualquier valor real en el rango $(0, 1)$, lo que permite interpretar la salida como una probabilidad.\n",
    "- Su derivada se calcula como: $$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z)).$$\n",
    "\n",
    "**Desventajas:**\n",
    "- No está centrada en cero, ya que $$\\sigma(0)=0.5,$$ lo que puede introducir un sesgo en la propagación de la información.\n",
    "- Puede saturarse para valores muy altos o muy bajos de $z$, lo que provoca gradientes muy pequeños.\n",
    "\n",
    "### Función ReLU (Rectified Linear Unit)\n",
    "\n",
    "La función ReLU se expresa como:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "**Ventajas:**\n",
    "- Es muy rápida y sencilla de calcular.\n",
    "- Activa únicamente las neuronas con $z>0$, lo que puede generar modelos más esparsos.\n",
    "- Evita la saturación en la parte positiva.\n",
    "\n",
    "**Desventajas:**\n",
    "- Para $z \\leq 0$, la derivada es cero, lo que puede llevar a que algunas neuronas se \"apaguen\" durante el entrenamiento.\n",
    "\n",
    "### Función tanh (Tangente Hiperbólica)\n",
    "\n",
    "La función tanh se define como:\n",
    "\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Propiedades:**\n",
    "- La salida se sitúa en el rango $(-1, 1)$.\n",
    "- Está centrada en cero: $$\\tanh(0)=0.$$ Por ejemplo, al comparar con la sigmoide, donde $$\\sigma(0)=0.5,$$ disponer de una salida de $0$ facilita que los valores positivos y negativos se distribuyan de forma equilibrada, favoreciendo el flujo del gradiente.\n",
    "- Su derivada es: $$\\tanh'(z) = 1 - \\tanh^2(z).$$\n",
    "\n",
    "**Ventajas:**\n",
    "- Al estar centrada en cero, evita el sesgo en las activaciones, lo que a menudo mejora la convergencia durante el entrenamiento.\n",
    "- Suele dar mejores resultados en capas ocultas.\n",
    "\n",
    "**Desventajas:**\n",
    "- Puede saturarse para valores extremos de $z$, reduciendo la magnitud del gradiente.\n",
    "\n",
    "### Función Softmax\n",
    "\n",
    "La función softmax se utiliza principalmente en la capa de salida para problemas de clasificación multiclase. Dado un vector de activaciones\n",
    "\n",
    "$$\n",
    "z = [z_1, z_2, \\dots, z_K],\n",
    "$$\n",
    "\n",
    "se define como:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}, \\quad i = 1, \\dots, K.\n",
    "$$\n",
    "\n",
    "**Propiedades:**\n",
    "- Convierte un conjunto de valores en una distribución de probabilidad, de forma que la suma de las salidas es $1$.\n",
    "\n",
    "**Ventajas:**\n",
    "- Facilita la interpretación de las salidas como probabilidades en problemas multiclase.\n",
    "\n",
    "**Desventajas:**\n",
    "- Puede ser sensible a la escala de los valores en $z$, requiriendo en ocasiones normalización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b0a32",
   "metadata": {},
   "source": [
    "\n",
    "## Ejemplo Práctico: Visualización de Funciones de Activación\n",
    "\n",
    "A continuación se muestra cómo se comportan las funciones sigmoide, ReLU y tanh. Con este ejemplo se puede visualizar la relación entre el valor $z$ y la salida de cada función, lo que ayuda a comprender mejor sus propiedades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0368136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rango de valores para z\n",
    "z_values = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Definir las funciones de activación\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Calcular las salidas para cada función\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "relu_values = relu(z_values)\n",
    "tanh_values = tanh(z_values)\n",
    "\n",
    "# Configurar la figura para visualizar las tres funciones\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(z_values, sigmoid_values, color='blue')\n",
    "plt.title('Función Sigmoide')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\\\sigma(z)$')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(z_values, relu_values, color='red')\n",
    "plt.title('Función ReLU')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('ReLU(z)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(z_values, tanh_values, color='green')\n",
    "plt.title('Función tanh')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('tanh(z)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec964ed6",
   "metadata": {},
   "source": [
    "\n",
    "::: tip Consejo\n",
    "En esta sección se ha explicado cómo se representa una neurona artificial mediante una combinación lineal de entradas y un sesgo, y cómo se transforma ese valor usando diversas funciones de activación.\n",
    "\n",
    "- La función sigmoide transforma cualquier valor real en un número entre $0$ y $1$, pero no está centrada en cero.\n",
    "- La función ReLU activa solo los valores positivos, lo que puede dar lugar a modelos esparsos, aunque en ocasiones algunas neuronas se \"apagan\".\n",
    "- La función tanh, al estar centrada en cero (ya que $\\tanh(0)=0$), favorece que los valores positivos y negativos se distribuyan de forma equilibrada, lo que ayuda al flujo del gradiente y mejora la convergencia.\n",
    "- La función softmax convierte un vector de activaciones en una distribución de probabilidad, lo que es útil para la clasificación multiclase.\n",
    "\n",
    "Se recomienda experimentar con diferentes valores de $z$ para observar cómo varían las salidas de cada función y determinar cuál se adapta mejor al problema en cuestión.\n",
    ":::\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
